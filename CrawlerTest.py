# scheduler.py
from Crawler.Realtor_Spider import RealtorSpider
from Service.Manage_DataBase import Manage_DataBase 
import time
def run_daily_update():
    spider = RealtorSpider()
    # 1. è·å–æ‰€æœ‰ç”¨æˆ·è‡ªå®šä¹‰çš„åŒºåŸŸ
    regions = spider.db.regions.find({"active": True})
    
    for region in regions:
        print(f"å¼€å§‹æŠ“å–åŒºåŸŸ: {region['name']}")
        # 2. æŠ“å–æ•°æ®
        data = spider.fetch_data(region["coords"])
        
        if data and data.get("Results"):
            # 3. è°ƒç”¨æ‚¨å·²å®ç°çš„å­˜å‚¨é€»è¾‘è¿›è¡Œä¿å­˜
            db_manager = Manage_DataBase()
            db_manager.update_listings(data["Results"])
            print(f"âœ… åŒºåŸŸ {region['name']} æ›´æ–°å®Œæˆ")
        else:
            print(f"ğŸ›‘ åŒºåŸŸ {region['name']} æŠ“å–å¤±è´¥ï¼Œè·³è¿‡")
def collect_data_for_region(region):
    spider = RealtorSpider(db_uri="mongodb://192.168.2.24:27017/")
    continue_fetching = True
    page = 1
    while continue_fetching:
        data, continue_fetching = spider.fetch_data(region["coords"], page=page)
        if data and data.get("Results"):
            db_manager = Manage_DataBase(db_uri="mongodb://192.168.2.24:27017/")
            db_manager.process_and_save_listings(data)
            print(f"âœ… åŒºåŸŸ {region['name']} æ›´æ–°å®Œæˆ")
            page += 1
        else:
            print(f"ğŸ›‘ åŒºåŸŸ {region['name']} æŠ“å–å¤±è´¥ï¼Œè·³è¿‡")
        time.sleep(10)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
if __name__ == "__main__":
    region={
        "name":"Toronto Downtown",
        "coords":{
        "lat_max": "43.65322",
        "lng_max": "-79.38794",
        "lat_min": "43.64513",
        "lng_min": "-79.3977"
        }
    }
    collect_data_for_region(region)
# é…åˆ Linux Crontab æˆ– Celery å®šæ—¶æ‰§è¡Œ